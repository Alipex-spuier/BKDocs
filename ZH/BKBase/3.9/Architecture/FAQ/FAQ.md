# FAQ

## 1. localTime 和 dtEventTime

localTime：处理时间，被平台处理的时间
- 清洗场景：清洗任务处理这条数据的时间
- 计算场景：计算模块处理这条数据的时间

dtEventTime：数据时间，平台内标记这条数据的时间
- 清洗场景：清洗配置中选择的时间字段
- 计算场景：计算模块记录数据的时间窗口。


## 2. _startTime_ 和 _endTime_

### 基本介绍

_startTime_：实时计算系统字段，窗口开始时间

_endTime_：实时计算系统字段，窗口结束时间

- 计算场景：滚动/滑动/累加/会话窗口类型实时计算节点结果表：
窗口开始时间与窗口结束时间分别为实时计算节点处理这条数据时标记该数据所在窗口的开始时间与结束时间
- 计算场景：无窗口类型实时计算节点结果表
无窗口类型实时节点的上游数据源为数据集成清洗的结果表：窗口开始时间与窗口结束时间相等，实际含义为 dtEventTime
无窗口类型实时节点的上游数据源为实时计算生成的结果表：窗口开始时间与窗口结束时间分别为上游实时计算节点处理这条数据时标记该数据所在窗口的开始时间与结束时间
- 计算场景：实时数据源与实时关联数据源进行静态关联处理的实时计算节点结果表
实时数据源为数据集成清洗的结果表：窗口开始时间与窗口结束时间相等，实际含义为实时数据源的 dtEventTime
实时数据源为实时计算生成的结果表：窗口开始时间与窗口结束时间分别为实时数据源的上游实时计算节点处理这条数据时标记该数据所在窗口的开始时间与结束时间
### 正确用法示例
在结果表中通过查看数据的窗口开始/结束时间字段值，确定该条计算结果数据所在的窗口
在实时计算节点的 SQL 分组语句（GROUP BY）中使用窗口开始/结束时间字段将上游的结果数据再次分组进行计算
将窗口开始/结束时间字段设置别名作为普通字段使用，如: SELECT `_startTime_` AS `startPos`

### 错误用法示例
直接选取窗口开始/结束时间字段，显示作为计算结果表字段。如：SELECT `_startTime_` , `_endTime_`
将普通字段设置别名为窗口开始/结束时间字段，覆盖该系统字段。如：SELECT `report_time` AS `_startTime_` 


## 3. 日志采集常见问题
### 无数据上报原因排查
请参照以下步骤排查。

- 检查目标机器上在 节点管理中 Agent 状态和 bkunifylogbeat 插件状态，如无插件，请联系本业务的业务运维安装插件
- 检查目标机器中的配置情况 grep {dataid} /usr/local/gse_bkte/plugins/etc/bkunifylogbeat/bkunifylogbeat*.conf 查看是否有包含 dataid 的配置文件。
- 检查采集器进程状态 ps -ef | grep bkunify
- 检查目标采集路径日志文件是否有数据更新
- 检查采集器监听文件的情况 lsof -p ${采集器 pid} | grep {file_name} 或者 lsof -c bkunifylogbea

### 上报 HDFS 文件
- 目前数据集成的采集方式中没有直接对接 HDFS，可将 HDFS 以文件的方式通过 日志采集 上报。

- HDFS 下载文件默认先创建临时文件，再重命名文件到目标文件名，为系统的 MOVE 事件。

- 针对 MOVE 事件，采集器默认不采集存量数据。

- 建议下载文件后, cp 复制到目标采集路径下。

### Nginx 使用 logrotate 日志切分
当 Nginx 使用 logrotate 切分日志时，配置需要使用 create，不支持 copytruncate。
### 接入异常原因排查
点击异常 IP 执行历史，查看最近的执行历史信息。