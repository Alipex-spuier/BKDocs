# DataDog Integrations Development Guide

Official Integrations: [https://github.com/DataDog/integrations-core.git](https://github.com/DataDog/integrations-core.git)

Community Integrations: [https://github.com/DataDog/integrations-extras.git](https://github.com/DataDog/integrations-extras.git)

If you do not find the corresponding Integrations in the above two channels, or do not meet your personal needs, you can modify or re-develop a DataDog Integrations yourself.

The monitoring platform supports plug-ins that comply with the [DataDog Integrations development specifications](https://docs.datadoghq.com/developers/integrations/new_check_howto/). Therefore, after development is completed, it can be used on the monitoring platform and can also contribute to the DataDog community.

### Environmental requirements

Python 2.7 or above

### Step 1: Install developer tools

Python 2 will no longer be supported in version 2.x, so the version is fixed at 1.4 here.

```bash
pip install "datadog-checks-dev[cli]"==1.4
```

### Step 2: Generate project scaffolding

To set the current directory as the working directory, first pull the code for integrations-extras.

```bash
git clone https://github.com/DataDog/integrations-extras.git
```

Then set the default path to the folder where integrations-extras is located.

```bash
ddev config set extras "/path/to/integrations-extras"
ddev config set repo extras
```

If we want to develop an Integrations that collects **operating system CPU** indicators, we name it `system_cpu`

The `ddev` command provides a scaffolding for quickly creating all the files and directory structures required for a complete Integrations. The creation command is as follows:

```bash
ddev create system_cpu
```

After executing the command and filling in simple information, a folder named `system_cpu` will be created in the `integrations-extras` directory. The directory structure is as follows:

```bash
system_cpu
├── assets
│ └── service_checks.json
├── datadog_checks
│ └── system_cpu
│ └── data
│ └── conf.yaml.example
│ ├── __about__.py
│ ├── __init__.py
│ └── system_cpu.py
│ └── __init__.py
├── tests
│ ├── __init__.py
│ ├── conftest.py
│ └── test_system_cpu.py
├── CHANGELOG.md
├── MANIFEST.in
├── README.md
├── manifest.json
├── metadata.csv
├── requirements-dev.txt
├── requirements.in
├── setup.py
└── tox.ini
```

### Step 3: Improve the collection logic

The collection logic needs to be defined in `system_cpu/datadog_checks/system_cpu/system_cpu.py`, requiring:

- Implement a `Check` class
- This class must inherit from the `AgentCheck` class
- This class must implement a method with the signature `check(self, instance)`
- Code must be compatible with both Python 2 and Python 3

The following is the code generated by the scaffolding. We need to improve the check method.

```python
from datadog_checks.base import AgentCheck


class SystemCpuCheck(AgentCheck):
     def check(self, instance):
         # Need to add collection logic here
         pass
```

#### Indicator naming convention

[https://docs.datadoghq.com/developers/metrics/#naming-custom-metrics](https://docs.datadoghq.com/developers/metrics/#naming-custom-metrics)

#### How to report an indicator

[https://docs.datadoghq.com/developers/metrics/agent_metrics_submission/?tab=count](https://docs.datadoghq.com/developers/metrics/agent_metrics_submission/?tab=count)

The following is the collection logic of the example

```python
#coding=utf-8

import psutil
from datadog_checks.base import AgentCheck


class SystemCpuCheck(AgentCheck):
     def check(self, instance):
         # If the instance configuration contains other tags, they also need to be included when reporting data.
         instance_tags = instance.get('tags', [])

         cpu_times = psutil.cpu_times(percpu=True)

         # data collection
         # gauge is the type
         # 'system.cpu.count' is the indicator name
         #len(cpu_times) is the indicator value
         # instance_tags is the dimension list
         self.gauge('system.cpu.count', len(cpu_times), tags=instance_tags)

         for i, cpu in enumerate(cpu_times):
             tags = instance_tags + ['core:{0}'.format(i)]
             for key, value in cpu._asdict().items():
                 self.rate('system.cpu.{0}'.format(key), 100.0 * value, tags=tags)
```

Since the third-party library `psutil` is used, the following dependency statement needs to be added to `system_cpu/requirements.in`.

```bash
psutil==5.6.2
```

### Step 4: Write tests

To verify the plugin logic, unit tests need to be written. The scaffolding has generated the test script `system_cpu/tests/test_system_cpu.py`, and you can start writing unit tests directly on it.

The following is a unit test example code for `system_cpu`:

```python
# coding=utf-8

import mock
import psutil

from datadog_checks.system_cpu import SystemCpuCheck

# mock data
MOCK_PSUTIL_CPU_TIMES = [
    psutil._psosx.scputimes(user=7877.29, nice=0.0, system=7469.72, idle=38164.81),
    psutil._psosx.scputimes(user=3826.74, nice=0.0, system=2701.6, idle=46981.39),
    psutil._psosx.scputimes(user=7486.51, nice=0.0, system=5991.36, idle=40031.88),
    psutil._psosx.scputimes(user=3964.85, nice=0.0, system=2862.37, idle=46682.5),
]

def test_check(aggregator, instance):
     check = SystemCpuCheck('system_cpu', {}, {})

     # mock
     psutil_mock = mock.MagicMock(return_value=MOCK_PSUTIL_CPU_TIMES)
     with mock.patch('datadog_checks.system_cpu.system_cpu.psutil.cpu_times', psutil_mock):
         check.check(instance)

     # Assert the quantity and value of reported data
     aggregator.assert_metric('system.cpu.count', value=4, count=1)

     for i in range(4):
         aggregator.assert_metric('system.cpu.user', count=1, tags=['core:{0}'.format(i)])
```

After the unit test is written, execute the following command to start the test program.

```bash
ddev test system_cpu
```

When the console displays the word `Passed!`, it means that the unit test passed.

> References: [https://docs.datadoghq.com/developers/integrations/new_check_howto/#writing-tests](https://docs.datadoghq.com/developers/integrations/new_check_howto/#writing-tests)

### Step 5: Complete other information

At this point, a working plug-in is born. However, if you want to further comply with community norms, you need to continue to improve the following documents.

- Third-party library dependencies used in collection logic: `requirements.in`

- Plug-in documentation [README.md](https://docs.datadoghq.com/developers/integrations/new_check_howto/#populate-the-readme)
- Plug-in configuration file example [conf.yaml.example](https://docs.datadoghq.com/developers/integrations/new_check_howto/#configuration-file)
- Plug-in meta information [manifest.json](https://docs.datadoghq.com/developers/integrations/new_check_howto/#manifest-file)
- Declaration of collection metrics [metadata.csv](https://docs.datadoghq.com/developers/integrations/new_check_howto/#metrics-metadata-file)

> Reference: [https://docs.datadoghq.com/developers/integrations/new_check_howto/#configuration](https://docs.datadoghq.com/developers/integrations/new_check_howto/#configuration)

### Step 6: Use in monitoring platform

Reference documentation:

* [Make DataDog plug-in](../ProductFeatures/integrations-metric-plugins/import_datadog_online.md)
* [How to make DataDog plug-in offline](../Dev/import_datadog_offline.md)
